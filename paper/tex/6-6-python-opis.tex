\subsubsection{Analiza danych}
Analizę danych przeprowadzono natomiast korzystając z Jupyter Notebook oraz języka Python 3.0. Zdecydowano się na taką zmianę ponieważ język Python przy użyciu biblioteki matplotlib jest doskonałym narzędziem do analizy danych poprzez ich wizualizację. Uznano, że w przypadku posiadanych danych oraz chęci porównania właściwości określonych klas kont będzie to odpowiednia metoda uzupełniająca przeprowadzenie analizy. Wizualizacja właściwości kont oraz grup do których należą może pomóc w znalezieniu odpowiednich charakterystyk łączących lub różnicujących badane konta. 
\par
Dla każdego kroku analizy stworzono osobny notatnik. Taka struktura sprawia, że w\,łatwy sposób istnieje możliwość ponownego użycia stworzonych mechanizmów analizy w\,przypadku rozszerzenie liczby danych lub chęci zanalizowania podobnych struktur danych z innych źródeł. 
\par
W przypadku analizy kont jako indeks użyto nazwy własnej konta. Jest to możliwe ponieważ ta informacja jest informacją publiczną. Nazwa własna konta mogła posłużyć jako indeks ponieważ zgodnie z założeniami systemu Twitter jest ona unikalna tak samo jak unikalny jest indeks numeryczny konta. Zdecydowano się na użycie nazwy konta zamiast indeksu numerycznego ponieważ jest on więcej mówiący i łatwiejszy dla przyswojenia przez człowieka dokonującego analizy wykresów. 
\par
W celu dokonania analizy połączeń pomiędzy kontami oraz pomiędzy kontami a\,udo- stępnianymi przez nie linkami do serwisów zewnętrznych użyto biblioteki Networkx\footnote{\url{https://networkx.github.io}}. Pozwala ona na efektowną wizualizację sieci, które były odpowiednim wyborem do przedstawienia tego typu zależności. 
\par
Na każdym z rysunków mających na celu ukazanie analizowanych właściwości za pomocą wykresów starano jak najlepiej pokazać ważne dla analizy cechy. Dlatego też zdecydowano się zakodować przy użyciu wybranych kolorów wszystkie klasy danych. Pozwala to odbiorcy wykresu, czyli czytelnikowi, na szybką analizę rysunku. W\,niektórych przypadkach dodano również wizualne odwzorowanie wielkości wartości danych, na przykład poprzez grubość łuku w sieci lub rozmiar punktu na wykresie, które nie odpowiadają bezpośrednio odpowiednim wartościom, jedynie przedstawiają różnicę w skali wielkości wartości. 

\subsubsection{Klasyfikacja danych}
Badanie klasyfikacji danych przeprowadzono na podstawie kodu udostępnionego przez twórców pracy Some like it hoax\footnote{\url{https://github.com/gabll/some-like-it-hoax}}. Zdecydowano się na taki ruch ponieważ chciano porównać czy metody przedstawione w tej pracy, które dały tak wysokie wyniki precyzji uda się odtworzyć na danych polskojęzycznych a przy tym udowodnić, że tego typu dane są możliwe do klasyfikacji i istnieje obiecująca ścieżka dalszych prac nad tematem automatycznej klasyfikacji nierzetelnych informacji w języku polskim w mediach społecznościowych. Jednak z powodu niedostępności odpowiednich funkcji API platformy Facebook nie było możliwe otrzymanie tego samego typu danych. Zdecydowano się więc na zebranie danych z innej platformy mediów społecznościowych która nieznacznie różni się charakterem interakcji użytkowników z publikowanymi treściami. 
\par
Aby mimo tej różnicy zachować jak największą spójność obu porównywanych w tym aspekcie badań podjęto decyzję użycia udostępnionego kodu implementującego mechanizmy klasyfikacji danych. Mimo to nieznaczne zmiany w kodzie były wymagane.
\par
Użyto implementacji dwóch algorytmów. Pierwszym z nich jest regresja logistyczna. Została ona dokonana z użyciem metod biblioteki sklearn\footnote{\url{https://scikit-learn.org/stable/}}. Natomiast druga z metod polega na przedstawieniu danych jako grafu gdzie wierzchołkami są zarówno posty jak i użytkownicy oraz propagowaniu wiedzy o wierzchołkach przy użyciu kroków algorytmu harmonicznego. Dokładniejszy opis użytych algorytmów znajduje się w podrozdziale \ref{klasyfikacja-algorytmy}. 